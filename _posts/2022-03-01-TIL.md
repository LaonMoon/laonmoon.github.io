---
title: "220301"
author: LaonMoon
date: 2022-03-01 12:25:00 +/-TTTT
categories: [Study,TIL]
tags: [TIL]
---

#### **Kogpt2 fine-tuning 성능 향상 고민**

##### [AI x Bookathon｜인공지능을 [수필 쓰는 작가]로 학습시켜보자!](https://jeinalog.tistory.com/25)

대략적인 방향을 잡을 수 있었던 고마운 글이다. 일단 이 글을 간략하게 정리해보자면, 약 70mb의 데이터를 모았지만 직접 데이터를 확인하며 결국 **5mb의 데이터**만 가지고 학습을 진행했다고 한다.

**tokenizer**는 mecab 기반의 tokenizer를 사용하였고 - 형태소 단위로 쪼갰다. 토크나이징을 모두 거친 후 각 토큰에 대한 id를 부여해 숫자 데이터로 변환하고 나면 모델에 입력될 수 있는 array 형태로의 변환이 끝나게 된다. 

-> 학습 과정 중에 "[PAD]"(공백)만 내뿜으며 발산하는 경우가 있었는데 learning rate를 학습 step 단계별로 순차적으로 decay를 해줌으로써 해결했다고 한다.

**학습 과정**은 5000step을 학습했다.(사전 학습 모델은 약 4000만 step을 학습!)

##### [2021 SKKU AI x Bookathon 대상 후기](https://l-yohai.github.io/AI-Bookathon-%EB%8C%80%EC%83%81-%ED%9B%84%EA%B8%B0/)

다음 글 역시 정리를 하여 참고해보고자 한다. 우선 **GPU**는 T4를 제공받아 진행되었다.

- **데이터 수집 및 전처리**
-> 160mb의 데이터를 수집(브런치 감성 에세이 탭, 문학광장 사이트의 글틴(명예의 전당), 신문사들의 신춘문예 당선작). 전처리는 KLUE에서 소개된 전처리 방법을 차용함.

- **주제 선정 및 Retrieval**
-> Open Domain Question Answering 과 Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks 논문을 보고 - Retrieval 을 통해 어떠한 키워드 및 주제에 유사도가 높은 데이터만을 학습시키면 해당 주제에서 어울리는 글을 생성해 낼 것이다는 아이디어를 떠올리게 되었다고 한다.

-> **Retrieval**
Open Domain Question Answering 과제에서는 Query 가 들어오면 Wiipedia 에서 주어진 쿼리와 유사한 Passage 들을 가져오는 Retrieval 시스템이 존재하고, 이와 유사하게 소주제를 쿼리로 하여 수집한 약 3만 건의 데이터에서 유사도가 높은 문장들을 가져오는 Retrieval 을 구현하게 되었다고 한다. TF-IDF, Dense Passage Retrieval 등 다양한 방법이 많았지만 bm25 를 사용한 Elastic Search 를 사용했다.

하지만 제대로 이루어지지 못했고, 직접 소주제를 대표하는 keyword를 일일이 뽑아내어 총 7개의 query를 만들어냈다고 한다. 그리고 유사도를 고려하여 데이터 샘플들을 모았다.(Top-k Data Retrieval)

- **모델 선정 및 학습**
**사용한 모델**은 [skt/ko-gpt-trinity-1.2B-v0.5](https://huggingface.co/skt/ko-gpt-trinity-1.2B-v0.5)이었다.

**학습 과정에서 out-of-memory 문제**가 발생함. Batch size를 1로 하여도 문제 발생. 따라서 24개의 Decoder Layer 중 12개의 Decoder Layer를 Freezing 시키고, Half Precision을 사용함. 또한 batch 크기를 최소한으로 했기 때문에 Accumulation step을 활용함.([참고자료](https://velog.io/@nawnoes/Pytorch%EB%A1%9C-%ED%81%B0-%EB%AA%A8%EB%8D%B8-%ED%95%99%EC%8A%B5%EC%8B%9C-%EC%96%B4%EB%96%BB%EA%B2%8C-%EB%B0%B0%EC%B9%98-%EC%82%AC%EC%9D%B4%EC%A6%88%EB%A5%BC-%EB%8A%98%EB%A6%B4%EC%88%98-%EC%9E%88%EC%9D%84%EA%B9%8C))

이렇게 각각의 keyword에 대해 retrieval로 가져온 500개의 sample들을 fine-tuning 시켰다.

- **수필 생성**
결과물은 prompt로 주어지는 텍스트, 그리고 generate method의 argument에 따라 매우 다른 결과들이 생성됨. - **Beam search 사용시 repetition problem 발생**은 **top-p sampling**방식을 이용하여 generate를 진행하는 것으로 해결.

```
gen_ids = model.generate(torch.tensor([input_ids]),
                         max_length=1024,
                         repetition_penalty=2.0,
                         pad_token_id=tokenizer.pad_token_id,
                         eos_token_id=tokenizer.eos_token_id,
                         bos_token_id=tokenizer.bos_token_id,
                         do_sample=True,
                         top_k=30,
                         top_p=0.95,
                         use_cache=True)
```

또한, Max Length를 길게 잡았을 때 생성된 텍스트에 일관성이 존재하지 않는 형상은 문장 생성을 짧게 하여 이어붙이는 방식을 사용함.

##### **나름대로의 내 해석**
앞서 정리한 두 팀의 학습 과정은 그렇게까지 많은 양의 데이터를 필요로 하지 않은 것 같다. 오히려 데이터의 질에 보다 집중하고 어떤 학습 과정을 거쳤느냐가 더 중요해 보인다.

- [한국어 자연어처리 1편_서브워드 구축(Subword Tokenizer, Mecab, huggingface VS SentencePiece)](https://keep-steady.tistory.com/37)


- 그래서 bpe와 형태소 분석기의 차이점이 뭘까? transformers에서는 bpe를 사용하는 것이 맞나? 내가 작성한 코드에서는 어떤 토크나이저를 쓰고 있는거지?
- transformers에 의존하지 않고 좀 더 low-level의 코드를 작성해야 할 필요성을 느꼈다. 정확히 어떤 구조로 - 예를 들면 어떤 search를 사용하며 돌아가고 있는지(?)부터 알아봐야 할듯.
-> **참고 자료들**(https://github.com/jskwak98/Bookathon3_Bookie_On_And_On/blob/main/text_generation/train.py, https://github.com/sohyeon98720/KoGPT2-finetuning, https://github.com/gyunggyung/KoGPT2-FineTuning
)
- 그래서 한정된 자원 안에서 최선의 결과를 이끌어내기 위해 각 요소들 간의 균형이 중요할 것 같기도 하고...(batch size나 learning rate이나 epoch 횟수나 layer의 개수나... 등등) 어떤 데이터를 넣을 것인지 또 얼마나 넣을 것인지 등도 고려해야 할 것 같다. 긴 글을 생성하려면 인풋 역시 반드시 길어야 하는지?
- 신춘문예 당선작을 활용해보는 건 생각도 못했었는데.. 꽤 괜찮을 것 같기도 하다.
- 생성 뿐 아니라 플롯 자체를 짤 때에도 도움을 받을 수 있을 것이다. 물론 그건 랜덤성으로 프로그램을 짜서 도출할 수도 있겠지만.. 스토리헬퍼?라는 서비스가 있었던 것 같다.